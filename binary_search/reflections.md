# Thoughts on Binary Search in General

First, some basic remarks. Binary search is an algorithm designed to find a target element in a sorted array, or return an appropriate consolation if that element does not exist in that array. It is absolutely crucial that the array be *sorted* in order for binary search to work. This will become clear when I describe how binary search works below.

Binary search can be contrasted with a simpler, less efficient, algorithm called *linear* search. Linear search functions exactly as its name would suggest: it searches for a target element in a sorted array in a linear fashion. Thus, linear search starts with the first element of the array and checks whether it is the target element. If it is, the algorithm returns the result. If it is not, linear search moves on to the next element in the array and repeats the process. (Note: the version of linear search that I have described is what Thomas M. Cormen labels *Better-Linear-Search* in his book *Algorithms Unlocked*. For Cormen, the simplest version of linear search simply loops over an entire array and *records* the index at which the target element was found, but *keeps going through the whole array* even after the target was found. My version of linear search returns as soon as the target is found.)

Linear search so described has a bad time complexity. For, in the worst case scenario, the element to be found is the *last* element in the array, meaning that the linear search algorithm had to go through the *entire array* element by element in order to find the target. Thus, for an array of *n* elements, linear search has a (worst-case) time complexity of O(*n*).

Binary search is more efficient. It relies crucially on the fact that the array is sorted, and uses this fact to break the problem down into smaller sub-problems. Instead of going through each element of the array and comparing it to the target until the target is found, binary search starts by finding the *middle* element of the array and comparing it to the target. If it just so happens to *be* the target, then this is the best case scenario and we are done. But if this middle element is *not* the target, then our next step is to see whether our target is *greater* or *less* than this middle element. If it is *greater* than the middle element, then we know that we can ignore every element in the array *up to* that middle element. For, since the array is sorted, *every* one of those elements will *also* be less than our target, and we know we will not find our target there. Therefore, we can ignore the entire first half of the array and search for our target in the second half, which features all the elements that are *also* greater than the middle element. Likewise, if our target is *less* than the middle element, then we know that we can ignore every element in the array *after* that middle element. For, since the array is sorted, *every* one of those elements will *also* be greater than our target, and we know we will not find our target there. Therefore, we can ignore the entire second half of the array and search for our target in the first half, which features all the elements that are *also* less than the middle element.

Binary search therefore *halves* the size of our initial array and searches for the target again with a new middle element: whatever is the middle element of the half of our original array that we are searching. It then performs the same operations as above, halving the new array to be searched repeatedly until either we find the target or learn that it is not in our array.

It should be clear that binary search, so described, is ripe for implementation as a *recursive function*. If the target element is not identical to the middle element of the initial array, decide which half the target element would have to belong in, and then *call binary search again on that half of the initial array*. Repeat this process until the target is found (or we learn that the target does not exist).

What is the time complexity of binary search? The answer is O(log *n*). To see this, think about the procedure described above: which each recursive call of the binary search algorithm, the array to be searched has been *halved* in size from the array on the previous call to the algorithm. The goal is to eventually get down to an array with a single element: our target. So how long does it take for an array with *n* elements to be repeatedly halved until it has just a single element? We can answer this by flipping the question: how long would it take for an array of a single element to end up with *n* elements if we kept doubling its size? The answer to *this* question is straightforward: *n* = 2 ^ *x*. We know what *n* is, since it's the number of elements in the array we are trying to construct, so we simply solve for *x*. In this case, *x* is log *n*. Why? Because logarithms are the *inverse* of exponents. That is, a logarithm is the power to which a number (in this case, 2) must be raised in order to produce some other number (in this case, *n*). Therefore, log *n* must be the value of *x*. We must take our array with a single element and double it log *n* times in order to generate our initial array. Therefore, to construct an array with *n* elements we must multiply 2 by itself log *n* times. But this is just the inverse process of *halving* an initial array until we get an array with a single element. Therefore, how many times do we have to halve our initial array to find our target element? Log *n* times. Therefore, binary search has a (worst-case) time complexity of O(log *n*).

# Thoughts on My Implementation of Binary Search

After submitting the code in `solution.rb` on Leetcode, I discovered that my code had a runtime of 94ms. At time of writing, this beat 86.69% of all submissions on Leetcode for binary search. My code also took up 213.9 MB of memory. At time of writing, this beat just 38.39% of all submissions on Leetcode for binary search. I know that my code could be improved in terms of memory allocation, but I am happy with its runtime, and suspect that many of the solutions that did better in terms of memory allocation did worse in terms of runtime.